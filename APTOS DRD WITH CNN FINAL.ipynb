{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 96\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "SAMPLE_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$1Recycle.Bin',\n",
       " '$RECYCLE.BIN',\n",
       " 'DumpStack.log.tmp',\n",
       " 'input',\n",
       " 'NPKey.txt',\n",
       " 'pagefile.sys',\n",
       " 'sample_submission.csv',\n",
       " 'System Volume Information',\n",
       " 'test.csv',\n",
       " 'test_images',\n",
       " 'train.csv',\n",
       " 'train_images']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(r'D:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3662\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(r'D:/train_images')))\n",
    "print(len(os.listdir(r'D:/test_images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3662, 2)\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_csv(r'D:/train.csv')\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1805\n",
       "2     999\n",
       "1     370\n",
       "4     295\n",
       "3     193\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n",
    "    \n",
    "    \"\"\"\n",
    "    Give a column in a dataframe,\n",
    "    this function takes a sample of each class and displays that\n",
    "    sample on one row. The sample size is the same as figure_cols which\n",
    "    is the number of columns in the figure.\n",
    "    Because this function takes a random sample, each time the function is run it\n",
    "    displays different images.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    categories = (df.groupby([col_name])[col_name].nunique()).index\n",
    "    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n",
    "                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n",
    "    # draw a number of images for each location\n",
    "    for i, cat in enumerate(categories):\n",
    "        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n",
    "        for j in range(0,figure_cols):\n",
    "            file=IMAGE_PATH + sample.iloc[j]['id_code'] + '.png'\n",
    "            im=cv2.imread(file)\n",
    "            ax[i, j].imshow(im, resample=True, cmap='gray')\n",
    "            ax[i, j].set_title(cat, fontsize=16)  \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_PATH = r'D:/train/' \n",
    "\n",
    "# draw_category_images('diagnosis',4, df_data, IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_code  diagnosis\n",
       "0  000c1434d8d7          2\n",
       "1  001639a390f0          4\n",
       "2  0024cdab0c1e          1\n",
       "3  002c21358ce6          0\n",
       "4  005b95c28852          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take a random sample of class 0 with size equal to num samples in class 1\n",
    "# df_0 = df_data[df_data['diagnosis'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n",
    "# # filter out class 1\n",
    "# df_1 = df_data[df_data['diagnosis'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n",
    "\n",
    "# # concat the dataframes\n",
    "# df_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n",
    "# # shuffle\n",
    "# df_data = shuffle(df_data)\n",
    "\n",
    "# df_data['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3295, 2)\n",
      "(367, 2)\n"
     ]
    }
   ],
   "source": [
    "# train_test_split\n",
    "\n",
    "# stratify=y creates a balanced validation set.\n",
    "y = df_data['diagnosis']\n",
    "\n",
    "df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1624\n",
       "2     899\n",
       "1     333\n",
       "4     265\n",
       "3     174\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    181\n",
       "2    100\n",
       "1     37\n",
       "4     30\n",
       "3     19\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "\n",
    "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
    "\n",
    "# now we create 2 folders inside 'base_dir':\n",
    "\n",
    "# train_dir\n",
    "    # a_no_tumor_tissue\n",
    "    # b_has_tumor_tissue\n",
    "\n",
    "# val_dir\n",
    "    # a_no_tumor_tissue\n",
    "    # b_has_tumor_tissue\n",
    "\n",
    "\n",
    "\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "\n",
    "\n",
    "# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n",
    "# Inside each folder we create seperate folders for each class\n",
    "\n",
    "# create new folders inside train_dir\n",
    "no_tumor_tissue = os.path.join(train_dir, 'no_drd')\n",
    "os.mkdir(no_tumor_tissue)\n",
    "has_tumor_tissue = os.path.join(train_dir, 'drd')\n",
    "os.mkdir(has_tumor_tissue)\n",
    "\n",
    "\n",
    "# create new folders inside val_dir\n",
    "no_tumor_tissue = os.path.join(val_dir, 'no_drd')\n",
    "os.mkdir(no_tumor_tissue)\n",
    "has_tumor_tissue = os.path.join(val_dir, 'drd')\n",
    "os.mkdir(has_tumor_tissue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drd', 'no_drd']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('base_dir/train_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.set_index('id_code', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of train and val images\n",
    "train_list = list(df_train['id_code'])\n",
    "val_list = list(df_val['id_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the train images\n",
    "\n",
    "for image in train_list:\n",
    "    \n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image + '.png'\n",
    "    # get the label for a certain image\n",
    "    target = df_data.loc[image,'diagnosis']\n",
    "    \n",
    "    # these must match the folder names\n",
    "    if target == 0:\n",
    "        label = 'no_drd'\n",
    "    if target != 0:\n",
    "        label = 'drd'\n",
    "    \n",
    "    # source path to image\n",
    "    src = os.path.join(r'D:/train_images', fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(train_dir, label, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the val images\n",
    "\n",
    "for image in val_list:\n",
    "    \n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image + '.png'\n",
    "    # get the label for a certain image\n",
    "    target = df_data.loc[image,'diagnosis']\n",
    "    \n",
    "    # these must match the folder names\n",
    "    if target == 0:\n",
    "        label = 'no_drd'\n",
    "    if target != 0:\n",
    "        label = 'drd'\n",
    "    \n",
    "\n",
    "    # source path to image\n",
    "    src = os.path.join(r'D:/train_images', fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(val_dir, label, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624\n",
      "1671\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('base_dir/train_dir/no_drd')))\n",
    "print(len(os.listdir('base_dir/train_dir/drd')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "# check how many val images we have in each folder\n",
    "\n",
    "print(len(os.listdir('base_dir/val_dir/no_drd')))\n",
    "print(len(os.listdir('base_dir/val_dir/drd')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'base_dir/train_dir'\n",
    "valid_path = 'base_dir/val_dir'\n",
    "test_path = r'D:/test_images'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 10\n",
    "val_batch_size = 10\n",
    "\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3295 images belonging to 2 classes.\n",
      "Found 367 images belonging to 2 classes.\n",
      "Found 367 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='categorical')\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 94, 94, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 92, 92, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 90, 90, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 45, 45, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 45, 45, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 43, 43, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 41, 41, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 39, 39, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,661,186\n",
      "Trainable params: 1,661,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.3\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drd': 0, 'no_drd': 1}\n"
     ]
    }
   ],
   "source": [
    "# Get the labels that are associated with each index\n",
    "print(val_gen.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "330/330 [==============================] - 1040s 3s/step - loss: 0.4894 - accuracy: 0.8286 - val_loss: 0.3983 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - 854s 3s/step - loss: 0.3996 - accuracy: 0.8210 - val_loss: 0.3490 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - 739s 2s/step - loss: 0.3883 - accuracy: 0.8304 - val_loss: 0.3895 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - 703s 2s/step - loss: 0.3802 - accuracy: 0.8232 - val_loss: 0.3549 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - 647s 2s/step - loss: 0.3730 - accuracy: 0.8274 - val_loss: 0.3547 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - 599s 2s/step - loss: 0.3669 - accuracy: 0.8284 - val_loss: 0.3441 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - 594s 2s/step - loss: 0.3508 - accuracy: 0.8312 - val_loss: 0.3405 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - 604s 2s/step - loss: 0.3592 - accuracy: 0.8197 - val_loss: 0.3415 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - 591s 2s/step - loss: 0.3710 - accuracy: 0.8195 - val_loss: 0.3393 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - 601s 2s/step - loss: 0.3327 - accuracy: 0.8425 - val_loss: 0.3486 - val_accuracy: 0.8420\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - 630s 2s/step - loss: 0.3616 - accuracy: 0.8238 - val_loss: 0.3385 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - 625s 2s/step - loss: 0.3368 - accuracy: 0.8394 - val_loss: 0.3636 - val_accuracy: 0.8420\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - 589s 2s/step - loss: 0.3501 - accuracy: 0.8337 - val_loss: 0.3386 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - 584s 2s/step - loss: 0.3546 - accuracy: 0.8301 - val_loss: 0.3424 - val_accuracy: 0.8447\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - 593s 2s/step - loss: 0.3464 - accuracy: 0.8300 - val_loss: 0.3418 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - 594s 2s/step - loss: 0.3323 - accuracy: 0.8395 - val_loss: 0.3427 - val_accuracy: 0.8365\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - 601s 2s/step - loss: 0.3377 - accuracy: 0.8378 - val_loss: 0.3460 - val_accuracy: 0.8447\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 18/20\n",
      "330/330 [==============================] - 598s 2s/step - loss: 0.3491 - accuracy: 0.8233 - val_loss: 0.3463 - val_accuracy: 0.8392\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 19/20\n",
      "330/330 [==============================] - 578s 2s/step - loss: 0.3432 - accuracy: 0.8298 - val_loss: 0.3526 - val_accuracy: 0.8447\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "Epoch 20/20\n",
      "330/330 [==============================] - 585s 2s/step - loss: 0.3375 - accuracy: 0.8334 - val_loss: 0.3456 - val_accuracy: 0.8420\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n"
     ]
    }
   ],
   "source": [
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=20, verbose=1,\n",
    "                   callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the metric names so we can use evaulate_generator\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.34557998180389404\n",
      "val_acc: 0.8419618606567383\n"
     ]
    }
   ],
   "source": [
    "# Here the best epoch will be used.\n",
    "\n",
    "\n",
    "\n",
    "val_loss, val_acc = \\\n",
    "model.evaluate_generator(test_gen, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 36s 98ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drd': 0, 'no_drd': 1}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how to check what index keras has internally assigned to each class. \n",
    "test_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_drd</th>\n",
       "      <th>drd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180965</td>\n",
       "      <td>0.819035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416962</td>\n",
       "      <td>0.583038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.331804</td>\n",
       "      <td>0.668196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.993109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449063</td>\n",
       "      <td>0.550937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_drd       drd\n",
       "0  0.180965  0.819035\n",
       "1  0.416962  0.583038\n",
       "2  0.331804  0.668196\n",
       "3  0.006891  0.993109\n",
       "4  0.449063  0.550937"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the predictions into a dataframe.\n",
    "# The columns need to be oredered to match the output of the previous cell\n",
    "\n",
    "df_preds = pd.DataFrame(predictions, columns=['no_drd', 'drd'])\n",
    "\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels\n",
    "y_true = test_gen.classes\n",
    "\n",
    "# Get the predicted labels as probabilities\n",
    "y_pred = df_preds['drd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8354061193044243"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels of the test images.\n",
    "\n",
    "test_labels = test_gen.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax returns the index of the max value in a row\n",
    "cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[  2  57]\n",
      " [  1 307]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the labels of the class indices. These need to match the \n",
    "# order shown above.\n",
    "cm_plot_labels = ['no_drd', 'drd']\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      no_drd       0.67      0.03      0.06        59\n",
      "         drd       0.84      1.00      0.91       308\n",
      "\n",
      "    accuracy                           0.84       367\n",
      "   macro avg       0.76      0.52      0.49       367\n",
      "weighted avg       0.81      0.84      0.78       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a classification report\n",
    "\n",
    "# For this to work we need y_pred as binary labels not as probabilities\n",
    "y_pred_binary = predictions.argmax(axis=1)\n",
    "\n",
    "report = classification_report(y_true, y_pred_binary,target_names=cm_plot_labels)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE A TEST SET PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete base_dir and it's sub folders to free up disk space.\n",
    "\n",
    "# shutil.rmtree('base_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CREATE A TEST FOLDER DIRECTORY STRUCTURE]\n",
    "\n",
    "# We will be feeding test images from a folder into predict_generator().\n",
    "# Keras requires that the path should point to a folder containing images and not\n",
    "# to the images themselves. That is why we are creating a folder (test_images) \n",
    "# inside another folder (test_dir).\n",
    "\n",
    "# test_dir\n",
    "    # test_images\n",
    "\n",
    "# create test_dir\n",
    "test_dir = 'test_dir'\n",
    "os.mkdir(test_dir)\n",
    "    \n",
    "# create test_images inside test_dir\n",
    "test_images = os.path.join(test_dir, 'test_images')\n",
    "os.mkdir(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_images']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the directory we created exists\n",
    "os.listdir('test_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer the test images into image_dir\n",
    "\n",
    "test_list = os.listdir(r'D:/test_images')\n",
    "\n",
    "for image in test_list:\n",
    "    \n",
    "    fname = image\n",
    "    \n",
    "    # source path to image\n",
    "    src = os.path.join(r'D:/test_images', fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(test_images, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the images are now in the test_images\n",
    "# Should now be 57458 images in the test_images folder\n",
    "\n",
    "len(os.listdir('test_dir/test_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1928 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_path ='test_dir'\n",
    "\n",
    "\n",
    "# Here we change the path to point to the test_images folder.\n",
    "\n",
    "test_gen = datagen.flow_from_directory(test_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928/1928 [==============================] - 108s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "num_test_images = 1928\n",
    "\n",
    "# make sure we are using the best epoch\n",
    "#model.load_weights('model.h5')\n",
    "\n",
    "predictions = model.predict_generator(test_gen, steps=num_test_images, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are the number of predictions correct?\n",
    "# Should be 57458.\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_drd</th>\n",
       "      <th>drd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.261342</td>\n",
       "      <td>0.738658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250476</td>\n",
       "      <td>0.749524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059153</td>\n",
       "      <td>0.940847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.302199</td>\n",
       "      <td>0.697801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.149701</td>\n",
       "      <td>0.850299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     no_drd       drd\n",
       "0  0.261342  0.738658\n",
       "1  0.250476  0.749524\n",
       "2  0.059153  0.940847\n",
       "3  0.302199  0.697801\n",
       "4  0.149701  0.850299"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the predictions into a dataframe\n",
    "\n",
    "df_preds = pd.DataFrame(predictions, columns=['no_drd', 'drd'])\n",
    "\n",
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_drd</th>\n",
       "      <th>drd</th>\n",
       "      <th>file_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.261342</td>\n",
       "      <td>0.738658</td>\n",
       "      <td>test_images\\0005cfc8afb6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250476</td>\n",
       "      <td>0.749524</td>\n",
       "      <td>test_images\\003f0afdcd15.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.059153</td>\n",
       "      <td>0.940847</td>\n",
       "      <td>test_images\\006efc72b638.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.302199</td>\n",
       "      <td>0.697801</td>\n",
       "      <td>test_images\\00836aaacf06.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.149701</td>\n",
       "      <td>0.850299</td>\n",
       "      <td>test_images\\009245722fa4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.030253</td>\n",
       "      <td>0.969747</td>\n",
       "      <td>test_images\\0ecaf177e85f.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.348876</td>\n",
       "      <td>0.651124</td>\n",
       "      <td>test_images\\0ed192cd67a2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.070498</td>\n",
       "      <td>0.929502</td>\n",
       "      <td>test_images\\0edc0c33e929.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.351120</td>\n",
       "      <td>0.648880</td>\n",
       "      <td>test_images\\0ee72c039036.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.124766</td>\n",
       "      <td>0.875234</td>\n",
       "      <td>test_images\\0ef1f5075178.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      no_drd       drd                    file_names\n",
       "0   0.261342  0.738658  test_images\\0005cfc8afb6.png\n",
       "1   0.250476  0.749524  test_images\\003f0afdcd15.png\n",
       "2   0.059153  0.940847  test_images\\006efc72b638.png\n",
       "3   0.302199  0.697801  test_images\\00836aaacf06.png\n",
       "4   0.149701  0.850299  test_images\\009245722fa4.png\n",
       "..       ...       ...                           ...\n",
       "95  0.030253  0.969747  test_images\\0ecaf177e85f.png\n",
       "96  0.348876  0.651124  test_images\\0ed192cd67a2.png\n",
       "97  0.070498  0.929502  test_images\\0edc0c33e929.png\n",
       "98  0.351120  0.648880  test_images\\0ee72c039036.png\n",
       "99  0.124766  0.875234  test_images\\0ef1f5075178.png\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This outputs the file names in the sequence in which \n",
    "# the generator processed the test images.\n",
    "test_filenames = test_gen.filenames\n",
    "\n",
    "# add the filenames to the dataframe\n",
    "df_preds['file_names'] = test_filenames\n",
    "\n",
    "df_preds.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
